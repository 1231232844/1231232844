{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1231232844/1231232844/blob/main/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aISe4pFfb_NQ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<!--- @wandbcode{sweeps-video} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM_7G5uib_NV"
      },
      "source": [
        "<img src=\"https://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{sweeps-video} -->\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<div><img /></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhMXDgJcb_Ne"
      },
      "source": [
        "## Step 2️: Initialize the Sweep\n",
        "\n",
        "Once you've defined the search strategy, it's time to set up something to implement it.\n",
        "\n",
        "W&B uses a Sweep Controller to manage sweeps on the cloud or locally across one or more machines. For this tutorial, you will use a sweep controller managed by W&B.\n",
        "\n",
        "While sweep controllers manage sweeps, the component that actually executes a sweep is known as a _sweep agent_.\n",
        "\n",
        "\n",
        ":::info\n",
        "By default, sweep controllers components are initiated on W&B's servers and sweep agents, the component that creates sweeps, are activated on your local machine.\n",
        ":::\n",
        "\n",
        "\n",
        "Within your notebook, you can activate a sweep controller with the `wandb.sweep` method. Pass your sweep configuration dictionary you defined earlier to the `sweep_config` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dza4Vyqzb_Ne"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cflu6L8cb_Ne"
      },
      "source": [
        "The `wandb.sweep` function returns a `sweep_id` that you will use at a later step to activate your sweep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOoR9YCKb_Ne"
      },
      "source": [
        ":::info\n",
        "On the command line, this function is replaced with\n",
        "```python\n",
        "wandb sweep config.yaml\n",
        "```\n",
        ":::\n",
        "\n",
        "For more information on how to create W&B Sweeps in a terminal, see the [W&B Sweep walkthrough](https://docs.wandb.com/sweeps/walkthrough).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7ig5O7b_Ne"
      },
      "source": [
        "## Step 3:  Define your machine learning code\n",
        "\n",
        "Before you execute the sweep,\n",
        "define the training procedure that uses the hyperparameter values you want to try. The key to integrating W&B Sweeps into your training code is to ensure that, for each training experiment, that your training logic can access the hyperparameter values you defined in your sweep configuration.\n",
        "\n",
        "In the proceeding code example, the helper functions `build_dataset`, `build_network`, `build_optimizer`, and `train_epoch` access the sweep hyperparameter configuration dictionary.\n",
        "\n",
        "Run the proceeding machine learning training code in your notebook. The functions define a basic fully-connected neural network in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kcxoQnsb_Ne"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        # If called by wandb.agent, as below,\n",
        "        # this config will be set by Sweep Controller\n",
        "        config = wandb.config\n",
        "\n",
        "        loader = build_dataset(config.batch_size)\n",
        "        network = build_network(config.fc_layer_size, config.dropout)\n",
        "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            avg_loss = train_epoch(network, loader, optimizer)\n",
        "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nDalYcCb_Nf"
      },
      "source": [
        "Within the `train` function, you will notice the following W&B Python SDK methods:\n",
        "* [`wandb.init()`](https://docs.wandb.com/library/init) – Initialize a new W&B run. Each run is a single execution of the training function.\n",
        "* [`wandb.config`](https://docs.wandb.com/library/config) – Pass sweep configuration with the hyperparameters you want to experiment with.\n",
        "* [`wandb.log()`](https://docs.wandb.com/library/log) – Log the training loss for each epoch.\n",
        "\n",
        "\n",
        "The proceeding cell defines four functions:\n",
        "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`.\n",
        "These functions are a standard part of a basic PyTorch pipeline,\n",
        "and their implementation is unaffected by the use of W&B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TahZDULDb_Nf"
      },
      "outputs": [],
      "source": [
        "def build_dataset(batch_size):\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    # download MNIST training dataset\n",
        "    dataset = datasets.MNIST(\".\", train=True, download=True,\n",
        "                             transform=transform)\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "        dataset, indices=range(0, len(dataset), 5))\n",
        "    loader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size)\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "def build_network(fc_layer_size, dropout):\n",
        "    network = nn.Sequential(  # fully-connected, single hidden layer\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(784, fc_layer_size), nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(fc_layer_size, 10),\n",
        "        nn.LogSoftmax(dim=1))\n",
        "\n",
        "    return network.to(device)\n",
        "\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(network, loader, optimizer):\n",
        "    cumu_loss = 0\n",
        "    for _, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ➡ Forward pass\n",
        "        loss = F.nll_loss(network(data), target)\n",
        "        cumu_loss += loss.item()\n",
        "\n",
        "        # ⬅ Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        wandb.log({\"batch loss\": loss.item()})\n",
        "\n",
        "    return cumu_loss / len(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pHGXFUwb_Nf"
      },
      "source": [
        "For more details on instrumenting W&B with PyTorch, see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSom-3cb_Nf"
      },
      "source": [
        "## Step 4: Activate sweep agents\n",
        "Now that you have your sweep configuration defined and a training script that can utilize those hyperparameter in an interactive way, you are ready to activate a sweep agent. Sweep agents are responsible for running an experiment with a set of hyperparameter values that you defined in your sweep configuration.\n",
        "\n",
        "Create sweep agents with the `wandb.agent` method. Provide the following:\n",
        "1. The sweep the agent is a part of (`sweep_id`)\n",
        "2. The function the sweep is supposed to run. In this example, the sweep will use the `train` function.\n",
        "3. (optionally) How many configs to ask the sweep controller for (`count`)\n",
        "\n",
        ":::tip\n",
        "You can start multiple sweep agents with the same `sweep_id`\n",
        "on different compute resources. The sweep controller ensures that they work together\n",
        "according to the sweep configuration you defined.\n",
        ":::\n",
        "\n",
        "The proceeding cell activates a sweep agent that runs the training function (`train`) 5 times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhtHTlkNb_Ng"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI6lpRMZb_Ng"
      },
      "source": [
        ":::info\n",
        "Since the `random` search method was specified in the sweep configuration, the sweep controller provides randomly-generated hyperparameter values.\n",
        ":::\n",
        "\n",
        "For more information on how to create W&B Sweeps in a terminal, see the [W&B Sweep walkthrough](https://docs.wandb.com/sweeps/walkthrough)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7T7ZuWKb_Ng"
      },
      "source": [
        "## Visualize Sweep Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snd_9QrJb_Ng"
      },
      "source": [
        "\n",
        "### Parallel Coordinates Plot\n",
        "This plot maps hyperparameter values to model metrics. It’s useful for honing in on combinations of hyperparameters that led to the best model performance.\n",
        "\n",
        "![](https://assets.website-files.com/5ac6b7f2924c652fd013a891/5e190366778ad831455f9af2_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695138341_image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhhLE9mRb_Ng"
      },
      "source": [
        "### Hyperparameter Importance Plot\n",
        "The hyperparameter importance plot surfaces which hyperparameters were the best predictors of your metrics.\n",
        "We report feature importance (from a random forest model) and correlation (implicitly a linear model).\n",
        "\n",
        "![](https://assets.website-files.com/5ac6b7f2924c652fd013a891/5e190367778ad820b35f9af5_s_194708415DEC35F74A7691FF6810D3B14703D1EFE1672ED29000BA98171242A5_1578695757573_image.png)\n",
        "\n",
        "These visualizations can help you save both time and resources running expensive hyperparameter optimizations by honing in on the parameters (and value ranges) that are the most important, and thereby worthy of further exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRbuySLnb_Nk"
      },
      "source": [
        "## Learn more about W&B Sweeps\n",
        "\n",
        "We created a simple training script and [a few flavors of sweep configs](https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion) for you to play with. We highly encourage you to give these a try.\n",
        "\n",
        "That repo also has examples to help you try more advanced sweep features like [Bayesian Hyperband](https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/us0ifmrf?workspace=user-lavanyashukla), and [Hyperopt](https://app.wandb.ai/wandb/examples-keras-cnn-fashion/sweeps/xbs2wm5e?workspace=user-lavanyashukla)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}